# -*- coding: utf-8 -*-
"""Carcino trial2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16Cnv_wvSaSyVjx_z31aOnAUKVBdaAs2l
"""

!pip install torch torch-geometric rdkit-pypi optuna scikit-learn

import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem, Descriptors
from google.colab import files
# Load your data
uploaded = files.upload()
carcinogenicity_data = pd.read_csv(list(uploaded.keys())[0])

# Load dataset (replace with the correct file path)
carcinogenicity_data = pd.read_csv('Carcinogenicity__BR_19082024.csv')



print(carcinogenicity_data.head())
smiles_column = carcinogenicity_data['Canonical SMILES']
targets = torch.tensor(carcinogenicity_data['Toxicity Value'].values, dtype=torch.float32)

def calculate_extended_descriptors(smiles):
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return [0] * 20  # Handling invalid molecules

    # Core and additional descriptors from RDKit
    descriptors = [
        Descriptors.MolWt(mol),
        Descriptors.NumHDonors(mol),
        Descriptors.NumHAcceptors(mol),
        Descriptors.RingCount(mol),
        Descriptors.TPSA(mol),
        Descriptors.NumRotatableBonds(mol),
        Descriptors.MolLogP(mol),
        Descriptors.MolMR(mol),
        Descriptors.qed(mol),
        Descriptors.BalabanJ(mol),
        Descriptors.BertzCT(mol),
        Descriptors.Chi0n(mol),
        Descriptors.Chi1n(mol),
        Descriptors.Kappa1(mol),
        Descriptors.Kappa2(mol),
        # Additional descriptors
        Descriptors.HallKierAlpha(mol),
        Descriptors.FpDensityMorgan1(mol),
        Descriptors.FpDensityMorgan2(mol),
        Descriptors.HeavyAtomMolWt(mol)
    ]

    return descriptors

import torch

def generate_features(smiles_list):
    features = []
    for smiles in smiles_list:
        mol = Chem.MolFromSmiles(smiles)

        # Morgan fingerprint
        fingerprint = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)
        fingerprint_array = torch.tensor(list(fingerprint), dtype=torch.float32)

        # Calculate extended descriptors
        descriptors = calculate_extended_descriptors(smiles)
        descriptors_array = torch.tensor(descriptors, dtype=torch.float32)

        # Combine fingerprint and descriptors
        combined_features = torch.cat([fingerprint_array, descriptors_array])
        features.append(combined_features)

    return torch.stack(features)

# Generate features
features = generate_features(smiles_column)

from torch_geometric.data import Data

def mol_to_graph(mol, drop_edge_prob=0.2):
    atoms = [atom.GetAtomicNum() for atom in mol.GetAtoms()]
    bonds = [bond.GetBondTypeAsDouble() for bond in mol.GetBonds()]

    edge_index = []
    for bond in mol.GetBonds():
        edge_index.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])
        edge_index.append([bond.GetEndAtomIdx(), bond.GetBeginAtomIdx()])

    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()
    x = torch.tensor(atoms, dtype=torch.float).view(-1, 1)

    edge_index, _ = dropout_edge(edge_index, p=drop_edge_prob)  # Drop edges
    return Data(x=x, edge_index=edge_index)

from torch_geometric.data import Data
from torch_geometric.utils import dropout_edge  # Import the dropout_edge function

def mol_to_graph(mol, drop_edge_prob=0.2):
    atoms = [atom.GetAtomicNum() for atom in mol.GetAtoms()]
    bonds = [bond.GetBondTypeAsDouble() for bond in mol.GetBonds()]

    edge_index = []
    for bond in mol.GetBonds():
        edge_index.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])
        edge_index.append([bond.GetEndAtomIdx(), bond.GetBeginAtomIdx()])

    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()
    x = torch.tensor(atoms, dtype=torch.float).view(-1, 1)

    # Apply edge dropout
    edge_index, _ = dropout_edge(edge_index, p=drop_edge_prob)

    return Data(x=x, edge_index=edge_index)


def create_data_objects(smiles_list, features, targets):
    data_list = []
    for i, smiles in enumerate(smiles_list):
        mol = Chem.MolFromSmiles(smiles)
        graph_data = mol_to_graph(mol)  # Convert molecule to graph data

        # Combine node features (x) with the molecular features (features[i])
        data = Data(
            x=graph_data.x,
            edge_index=graph_data.edge_index,
            y=torch.tensor([targets[i]], dtype=torch.float32),
            features=features[i]  # Optional: Keep this if you need to reference it separately
        )
        data_list.append(data)

    return data_list



# Generate data objects
data_list = create_data_objects(smiles_column, features, targets)

# Updated version of the focal_loss function
def focal_loss(predictions, targets, alpha=0.25, gamma=2.0):
    # Compute binary cross-entropy loss for each sample
    bce_loss = F.binary_cross_entropy_with_logits(predictions, targets, reduction='none')

    # Calculate p_t, which is the probability assigned to the true class
    p_t = torch.where(targets == 1, predictions, 1 - predictions)

    # Compute the focal loss
    focal_loss_value = alpha * (1 - p_t) ** gamma * bce_loss
    return focal_loss_value.mean()

import torch
import torch.nn as nn
from torch_geometric.nn import GATConv, TopKPooling, global_mean_pool

# Define the GNN model architecture
class EnhancedHybridGNNModel(nn.Module):
    def __init__(self, feature_size, gnn_hidden_channels, mlp_hidden_channels, dropout_rate, num_gnn_layers):
        super(EnhancedHybridGNNModel, self).__init__()

        # Initialize GNN layers and pooling layers
        self.gnn_layers = nn.ModuleList()
        self.pooling_layers = nn.ModuleList()

        # Create GNN and pooling layers for each layer
        for i in range(num_gnn_layers):
            in_channels = 1 if i == 0 else gnn_hidden_channels
            self.gnn_layers.append(
                nn.Sequential(
                    GATConv(in_channels, gnn_hidden_channels),
                    nn.BatchNorm1d(gnn_hidden_channels),
                    nn.ReLU(),
                    nn.Dropout(p=dropout_rate)
                )
            )
            self.pooling_layers.append(TopKPooling(gnn_hidden_channels, ratio=0.8))

        # Define MLP (Fully Connected Layer) to output a single logit for binary classification
        self.mlp = nn.Sequential(
            nn.Linear(gnn_hidden_channels + feature_size, mlp_hidden_channels),
            nn.BatchNorm1d(mlp_hidden_channels),
            nn.ReLU(),
            nn.Dropout(p=dropout_rate),
            nn.Linear(mlp_hidden_channels, 1)  # Output a single logit for binary classification
        )

    # Forward method with print statements
    def forward(self, graph):
        # Extract graph data from the input
        x, edge_index, batch = graph.x, graph.edge_index, graph.batch

        # Apply GNN layers and pooling layers
        for conv, pool in zip(self.gnn_layers, self.pooling_layers):
            x = conv(x, edge_index)
            x, edge_index, _, batch, _ = pool(x, edge_index, batch=batch)

        # Print the shape after GNN layers
        print("Shape after GNN layers:", x.shape)

        # Apply global pooling to get a single feature vector for each graph (sample)
        x = global_mean_pool(x, batch)

        # Print the shape after global pooling
        print("Shape after global pooling:", x.shape)

        # Concatenate additional features and ensure they match batch size
        features = graph.features.view(x.size(0), -1)
        x = torch.cat([x, features], dim=1)

        # Apply MLP to get the final output (one logit per sample)
        x = self.mlp(x)

        # Print the shape after MLP
        print("Shape after MLP:", x.shape)

        # Return the logit with the shape [batch_size, 1]
        return x.view(-1, 1)

# Define the loss function
loss_fn = nn.BCEWithLogitsLoss()

optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)  # L2 regularization

from torch.optim.lr_scheduler import CosineAnnealingLR

scheduler = CosineAnnealingLR(optimizer, T_max=50)

class EarlyStopping:
    def __init__(self, patience=5, delta=0):
        self.patience = patience
        self.delta = delta
        self.best_loss = float('inf')
        self.epochs_without_improvement = 0

    def step(self, val_loss):
        if val_loss < self.best_loss - self.delta:
            self.best_loss = val_loss
            self.epochs_without_improvement = 0
        else:
            self.epochs_without_improvement += 1

        return self.epochs_without_improvement >= self.patience

early_stopping = EarlyStopping(patience=5)

# Define the device (GPU if available, otherwise CPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Move the model to the device
model = model.to(device)

# Adding print statements within the train_model function to display the shapes before calculating the loss

def train_model(model, train_loader, optimizer, loss_fn):
    model.train()  # Set the model to training mode
    for batch in train_loader:
        # Move the entire batch to the device
        batch = batch.to(device)

        # Forward pass through the model
        predictions = model(batch)  # Output should have size (batch_size, 1)

        # Ensure the target is reshaped and converted to float
        targets = batch.y.unsqueeze(1).float().to(device)  # Shape should be (batch_size, 1)

        # Print shapes before calculating loss
        print(f"Predictions shape: {predictions.shape}")  # Print the shape of predictions
        print(f"Targets shape: {targets.shape}")  # Print the shape of targets

        # Compute the loss (predictions and targets should both be of the same shape)
        loss = loss_fn(predictions, targets)

        # Zero gradients, backpropagate, and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

# The print statements will provide insight into the shape mismatch issues just before the loss calculation.

def evaluate_model(model, val_loader):
    model.eval()  # Set the model to evaluation mode
    val_loss = 0
    with torch.no_grad():  # Disable gradient calculations
        for batch in val_loader:
            batch = batch.to(device)
            features = batch.features.to(device)
            targets = batch.y.unsqueeze(1).float().to(device)  # Ensure targets are (batch_size, 1)

            # Forward pass
            predictions = model(batch, features)

            # Reshape predictions to match target shape
            predictions = predictions.view(-1, 1).detach()  # Reshape and detach

            # Compute loss
            val_loss += focal_loss(predictions, targets).item()

    # Return the average validation loss
    return val_loss / len(val_loader)

# Assuming you already have model, optimizer, scheduler, early_stopping defined
for epoch in range(epochs):
    # Train the model using the train_model function
    train_model(model, train_loader, optimizer, focal_loss)  # Train with focal loss

    # Validate the model
    val_loss = evaluate_model(model, val_loader)  # Validation loss

    # Adjust learning rate
    scheduler.step()  # Adjust learning rate based on the schedule

    # Check for early stopping
    if early_stopping.step(val_loss):
        print("Early stopping triggered.")
        break

# K-Fold cross-validation
final_accuracy_list = []
kf = KFold(n_splits=5, shuffle=True, random_state=42)

for train_index, test_index in kf.split(features):
    train_loader = DataLoader(create_data_objects(smiles_column[train_index], features[train_index], targets[train_index]), batch_size=32, shuffle=True)
    test_loader = DataLoader(create_data_objects(smiles_column[test_index], features[test_index], targets[test_index]), batch_size=32, shuffle=False)

    # Train for a set number of epochs
    for epoch in range(30):
        train_model(best_model, train_loader, optimizer, focal_loss)

    # Evaluate the model
    accuracy = evaluate_model(best_model, test_loader)
    final_accuracy_list.append(accuracy)

print(f"Final accuracy after cross-validation: {np.mean(final_accuracy_list)}")

torch.save(best_model.state_dict(), 'best_gnn_model.pth')

new_data = pd.read_csv('Validation_dataset.csv')
new_features = generate_features(new_data['Canonical SMILES'])

# Create DataLoader for new data
new_loader = DataLoader(create_data_objects(new_data['Canonical SMILES'], new_features, [0] * len(new_data)), batch_size=32, shuffle=False)

best_model.eval()
predictions = []

with torch.no_grad():
    for batch in new_loader:
        batch = batch.to(device)
        features = batch.features.to(device)

        preds = best_model(batch, features).squeeze(1)
        preds = (preds > 0.5).float()  # Apply threshold
        predictions.extend(preds.cpu().numpy())

# Add predictions to the new data
new_data['Predicted_Carcinogenicity'] = predictions

# Save predictions
new_data.to_csv('Predicted_Carcinogenicity_Compounds.csv', index=False)
