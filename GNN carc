# -*- coding: utf-8 -*-
"""Carcino trial2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16Cnv_wvSaSyVjx_z31aOnAUKVBdaAs2l
"""

!pip install torch torch-geometric rdkit-pypi optuna scikit-learn

import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem, Descriptors
from google.colab import files
import torch
# Load your data
uploaded = files.upload()
carcinogenicity_data = pd.read_csv(list(uploaded.keys())[0])

# Load dataset (replace with the correct file path)
carcinogenicity_data = pd.read_csv('Carcinogenicity__BR_19082024.csv')



print(carcinogenicity_data.head())
smiles_column = carcinogenicity_data['Canonical SMILES']
targets = torch.tensor(carcinogenicity_data['Toxicity Value'].values, dtype=torch.float32)

def calculate_extended_descriptors(smiles):
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return [0] * 20  # Handling invalid molecules

    # Core and additional descriptors from RDKit
    descriptors = [
        Descriptors.MolWt(mol),
        Descriptors.NumHDonors(mol),
        Descriptors.NumHAcceptors(mol),
        Descriptors.RingCount(mol),
        Descriptors.TPSA(mol),
        Descriptors.NumRotatableBonds(mol),
        Descriptors.MolLogP(mol),
        Descriptors.MolMR(mol),
        Descriptors.qed(mol),
        Descriptors.BalabanJ(mol),
        Descriptors.BertzCT(mol),
        Descriptors.Chi0n(mol),
        Descriptors.Chi1n(mol),
        Descriptors.Kappa1(mol),
        Descriptors.Kappa2(mol),
        # Additional descriptors
        Descriptors.HallKierAlpha(mol),
        Descriptors.FpDensityMorgan1(mol),
        Descriptors.FpDensityMorgan2(mol),
        Descriptors.HeavyAtomMolWt(mol)
    ]

    return descriptors

import torch

def generate_features(smiles_list):
    features = []
    for smiles in smiles_list:
        mol = Chem.MolFromSmiles(smiles)

        # Morgan fingerprint
        fingerprint = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)
        fingerprint_array = torch.tensor(list(fingerprint), dtype=torch.float32)

        # Calculate extended descriptors
        descriptors = calculate_extended_descriptors(smiles)
        descriptors_array = torch.tensor(descriptors, dtype=torch.float32)

        # Combine fingerprint and descriptors
        combined_features = torch.cat([fingerprint_array, descriptors_array])
        features.append(combined_features)

    return torch.stack(features)

# Generate features
features = generate_features(smiles_column)

from torch_geometric.data import Data

def mol_to_graph(mol, drop_edge_prob=0.2):
    atoms = [atom.GetAtomicNum() for atom in mol.GetAtoms()]
    bonds = [bond.GetBondTypeAsDouble() for bond in mol.GetBonds()]

    edge_index = []
    for bond in mol.GetBonds():
        edge_index.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])
        edge_index.append([bond.GetEndAtomIdx(), bond.GetBeginAtomIdx()])

    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()
    x = torch.tensor(atoms, dtype=torch.float).view(-1, 1)

    edge_index, _ = dropout_edge(edge_index, p=drop_edge_prob)  # Drop edges
    return Data(x=x, edge_index=edge_index)

from torch_geometric.data import Data
from torch_geometric.utils import dropout_edge  # Import the dropout_edge function

def mol_to_graph(mol, drop_edge_prob=0.2):
    atoms = [atom.GetAtomicNum() for atom in mol.GetAtoms()]
    bonds = [bond.GetBondTypeAsDouble() for bond in mol.GetBonds()]

    edge_index = []
    for bond in mol.GetBonds():
        edge_index.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])
        edge_index.append([bond.GetEndAtomIdx(), bond.GetBeginAtomIdx()])

    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()
    x = torch.tensor(atoms, dtype=torch.float).view(-1, 1)

    # Apply edge dropout
    edge_index, _ = dropout_edge(edge_index, p=drop_edge_prob)

    return Data(x=x, edge_index=edge_index)


def create_data_objects(smiles_list, features, targets):
    data_list = []
    for i, smiles in enumerate(smiles_list):
        mol = Chem.MolFromSmiles(smiles)
        graph_data = mol_to_graph(mol)  # Convert molecule to graph data

        # Combine node features (x) with the molecular features (features[i])
        data = Data(
            x=graph_data.x,
            edge_index=graph_data.edge_index,
            y=torch.tensor([targets[i]], dtype=torch.float32),
            features=features[i]  # Optional: Keep this if you need to reference it separately
        )
        data_list.append(data)

    return data_list



# Generate data objects
data_list = create_data_objects(smiles_column, features, targets)

# Updated version of the focal_loss function
def focal_loss(predictions, targets, alpha=0.25, gamma=2.0):
    # Compute binary cross-entropy loss for each sample
    bce_loss = F.binary_cross_entropy_with_logits(predictions, targets, reduction='none')

    # Calculate p_t, which is the probability assigned to the true class
    p_t = torch.where(targets == 1, predictions, 1 - predictions)

    # Compute the focal loss
    focal_loss_value = alpha * (1 - p_t) ** gamma * bce_loss
    return focal_loss_value.mean()

import torch
from torch.nn import Linear, ReLU, Dropout
import torch.nn.functional as F
from torch_geometric.nn import GATConv, global_mean_pool
from torch.nn import BatchNorm1d # Import BatchNorm1d from torch.nn

class EnhancedHybridGNNModel(torch.nn.Module):
    def __init__(self, num_features, num_classes, num_gnn_layers=2):
        super(EnhancedHybridGNNModel, self).__init__()
        self.num_gnn_layers = num_gnn_layers

        self.gnn_layers = torch.nn.ModuleList()

        for i in range(self.num_gnn_layers):
            in_channels = num_features if i == 0 else 64
            self.gnn_layers.append(GATConv(in_channels, 64, heads=1))
            self.gnn_layers.append(BatchNorm1d(64)) # Use BatchNorm1d from torch.nn
            self.gnn_layers.append(ReLU())
            self.gnn_layers.append(Dropout(p=0.3))

        self.lin1 = Linear(64, 32)
        self.lin2 = Linear(32, 1)  # Output size changed to 1

    def forward(self, batch):
        x = batch.x
        edge_index = batch.edge_index

        j = 0
        for i in range(self.num_gnn_layers):
            x = self.gnn_layers[j](x, edge_index)
            x = self.gnn_layers[j+1](x)
            x = self.gnn_layers[j+2](x)
            x = self.gnn_layers[j+3](x)
            j += 4

        x = global_mean_pool(x, batch.batch)

        x = self.lin1(x)
        x = F.relu(x)
        x = F.dropout(x, p=0.5, training=self.training)
        x = self.lin2(x)

        return x

# Define the loss function
loss_fn = nn.BCEWithLogitsLoss()

# Instantiate the model
feature_size = 128  # Example feature size
num_classes = 2 # Example number of classes
num_gnn_layers = 3  # Example number of GNN layers

# Create an instance of your EnhancedHybridGNNModel
model = EnhancedHybridGNNModel(feature_size, num_classes, num_gnn_layers)

# Define the optimizer with model parameters
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)  # L2 regularization

from torch.optim.lr_scheduler import CosineAnnealingLR

scheduler = CosineAnnealingLR(optimizer, T_max=50)

class EarlyStopping:
    def __init__(self, patience=5, delta=0):
        self.patience = patience
        self.delta = delta
        self.best_loss = float('inf')
        self.epochs_without_improvement = 0

    def step(self, val_loss):
        if val_loss < self.best_loss - self.delta:
            self.best_loss = val_loss
            self.epochs_without_improvement = 0
        else:
            self.epochs_without_improvement += 1

        return self.epochs_without_improvement >= self.patience

early_stopping = EarlyStopping(patience=5)

# Define the device (GPU if available, otherwise CPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Move the model to the device
model = model.to(device)

def train_model(model, train_loader, optimizer, loss_fn):
    model.train()  # Set the model to training mode
    for batch in train_loader:
        # Move the entire batch to the device
        batch = batch.to(device)

        # Forward pass through the model
        predictions = model(batch)  # Output should have size (batch_size, 1)

        # Ensure the target is reshaped and converted to float
        targets = batch.y.unsqueeze(1).float().to(device)  # Shape should be (batch_size, 1)

        # Print shapes before calculating loss
        print(f"Predictions shape: {predictions.shape}")  # Print the shape of predictions
        print(f"Targets shape: {targets.shape}")  # Print the shape of targets

        # Compute the loss (predictions and targets should both be of the same shape)
        loss = loss_fn(predictions, targets)

        # Zero gradients, backpropagate, and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

def evaluate_model(model, val_loader):
    model.eval()  # Set the model to evaluation mode
    val_loss = 0
    with torch.no_grad():  # Disable gradient calculations
        for batch in val_loader:
            batch = batch.to(device)
            targets = batch.y.unsqueeze(1).float().to(device)  # Ensure targets are (batch_size, 1)

            # Forward pass
            predictions = model(batch)

            # Reshape predictions to match target shape
            predictions = predictions.view(-1, 1).detach()  # Reshape and detach

            # Compute loss
            val_loss += focal_loss(predictions, targets).item()

    # Return the average validation loss
    return val_loss / len(val_loader)

from torch_geometric.loader import DataLoader  # Change from data to loader

# Assuming you have a dataset, let's call it 'train_dataset'
# For example, if you are working with graphs, train_dataset could be a list of graphs
# Example: train_dataset = [graph1, graph2, ...]

# Ensure you have loaded your dataset correctly
# train_dataset should be a list of graph objects or a PyTorch Geometric dataset
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

# Now you can use the DataLoader for training

epochs = 50 # Add this line to define the number of epochs
for epoch in range(epochs):
    # Train the model using the train_model function
    train_model(model, train_loader, optimizer, focal_loss)  # Train with focal loss

    # Validate the model
    val_loss = evaluate_model(model, val_loader)  # Validation loss

    # Adjust learning rate
    scheduler.step()  # Adjust learning rate based on the schedule

    # Check for early stopping
    if early_stopping.step(val_loss):
        print("Early stopping triggered.")
        break





# K-Fold cross-validation
final_accuracy_list = []
kf = KFold(n_splits=5, shuffle=True, random_state=42)

for train_index, test_index in kf.split(features):
    train_loader = DataLoader(create_data_objects(smiles_column[train_index], features[train_index], targets[train_index]), batch_size=32, shuffle=True)
    test_loader = DataLoader(create_data_objects(smiles_column[test_index], features[test_index], targets[test_index]), batch_size=32, shuffle=False)

    # Train for a set number of epochs
    for epoch in range(30):
        train_model(best_model, train_loader, optimizer, focal_loss)

    # Evaluate the model
    accuracy = evaluate_model(best_model, test_loader)
    final_accuracy_list.append(accuracy)

print(f"Final accuracy after cross-validation: {np.mean(final_accuracy_list)}")

torch.save(best_model.state_dict(), 'best_gnn_model.pth')

new_data = pd.read_csv('Validation_dataset.csv')
new_features = generate_features(new_data['Canonical SMILES'])

# Create DataLoader for new data
new_loader = DataLoader(create_data_objects(new_data['Canonical SMILES'], new_features, [0] * len(new_data)), batch_size=32, shuffle=False)

best_model.eval()
predictions = []

with torch.no_grad():
    for batch in new_loader:
        batch = batch.to(device)
        features = batch.features.to(device)

        preds = best_model(batch, features).squeeze(1)
        preds = (preds > 0.5).float()  # Apply threshold
        predictions.extend(preds.cpu().numpy())

# Add predictions to the new data
new_data['Predicted_Carcinogenicity'] = predictions

# Save predictions
new_data.to_csv('Predicted_Carcinogenicity_Compounds.csv', index=False)
